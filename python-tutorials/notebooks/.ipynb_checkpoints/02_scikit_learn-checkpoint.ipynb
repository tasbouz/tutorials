{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Tutorial - Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Terminology\n",
    "\n",
    "Each row is an observation (or sample, example, instance, record). <br>\n",
    "Each column is a feature (or predictor, attribute, independent variable, input, regressor, covariate)<br>\n",
    "Each predicted value is the response. (or target)<br>\n",
    "Features and targets must: \n",
    "    1. Be separate objects, \n",
    "    2. Be numeric, \n",
    "    3. Be NumPy arrays (or pandas dataframe) \n",
    "    4. Have specific shapes:\n",
    "         * X = FEATURES: (rows, columns)   2-DIM MATRIX\n",
    "         * Y = TARGET: (rows)              1-DIM VECTOR\n",
    "\n",
    "<br>\n",
    "**USUAL STEPS FOR ML ALGORITHM**\n",
    "\n",
    "* STEP 1: Import the class we plan to use \n",
    "* STEP 2: Instantiate the classifier (Here we can specify all the arguments of the classifier)\n",
    "* STEP 3: Fit the data to the model (Here the method changes the classifier in-ace, no need to assign it to a variable)\n",
    "* STEP 4: Predict New values (Here we feed the classifier with new_data, i.e a Numpy array with ALL the features)\n",
    "* STEP 5: Model Evaluation (We can check accuracy of the model or in between models)\n",
    "     - First Way: Train the model on the entire dataset, and then use the model on the dataset to check its accuracy (NOT OPTIMAL)\n",
    "     - Second way: Train-Test Split (We split the data to train dataset and test dataset)   (OPTIMAL)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - K-NEIGHBORS CLASSIFIER\n",
    "\n",
    "Searches for K nearest neighbors (Euclidean Distance) from the point we want to predict, and gives as a result the most frequent neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: Import the class we plan to use \n",
    "from sklearn.neighbors import KNeighborsClassifirer                \n",
    "\n",
    "# STEP 2: Instantiate the classifier. \n",
    "clf = KNeighborsClassifirer(n_neighbors=5,              # n_neighbors: How many other points to take into account             \n",
    "                            weight_options='uniform')   # weight_options: The weights to put to each point around the one we deal with. \n",
    "                                                        # Uniform is the default, which everything takes the same weight. \n",
    "                                                        # Another one is distance which nearest neighbors take higher weights\n",
    "\n",
    "# (PRE)STEP 5: The Second way Train-Test Split (We split the data to train dataset and test dataset)   (OPTIMAL)  \n",
    "from sklearn.cross_validation import train_test_split                         \n",
    "\n",
    "# It splits the data to train and test datasets. \n",
    "# IMPORTANT:  Then we perform the whole model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,                                         \n",
    "                                                    test_size=0.5,        # Indicates the percentage of test to train dataset (here half-half, default 25/75)\n",
    "                                                    random_state=1)       # Model splits dataset in a random way, unless we specify the state (here we specified to 1st way)\n",
    "                                                    stratify=y            # Equal proportions of different set of targets in the training and test set\n",
    "\n",
    "# STEP 3: Fit the train dataset to the model           \n",
    "clf.fit(X_train,y_train)                    \n",
    "\n",
    "# STEP 4: Predict New values (We can also put here completely new values)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Gives the probability of each class of targets to be the correct one. \n",
    "# The classifier predicts class 1 when the probability is higher than the threshold (by default is 50%)\n",
    "y_prob = clf.predict_proba(X_test)  \n",
    "\n",
    "# OPTIONAL STEP: Serialize the classifier with pickle, so you don't need to train it over and over again. \n",
    "# Now, every time you open the file the classifier will be there, ready to use)\n",
    "import pickle \n",
    "\n",
    "# We create a variable to save the classifier\n",
    "with open(\"K-NeighborsClassifier.pickle\",\"wb\") as tmp:      \n",
    "    pickle.dump(clf,tmp)                                     # We save the classifier clf to save_classifier. wb = write bytes\n",
    "    tmp.close()                                              # Closes the process\n",
    "\n",
    "# We create a variable to load the saved pickle classifier. rb = read bytes\n",
    "with open(\"K-NeighborsClassifier.pickle\",\"rb\") as tmp:       \n",
    "    clf = pickle.load(tmp)                                   # We load the classifier to clf in order to use it\n",
    "\n",
    "\n",
    "# STEP 5: Model Evaluation\n",
    "# It's a good tactic to always compare the accuracy of the model with the NULL ACCURACY which is the accuracy that could be achieved by always predicting the most frequent class. This is the \"base accuracy\" (dumbest model). \n",
    "# No actual code for that, use pandas and find it.\n",
    "\n",
    "from sklearn import metrics               \n",
    "\n",
    "# 1st Way: accuracy_score(): UPSIDES: Easy and fast. DOWNSIDES: Does not tell the underlying distribution of responses AND the type of errors the classifier does\n",
    "# Model Evaluation Metric: Accuracy - Compares the prediction with the actual value. 1=perfect, 0=bad. \n",
    "metrics.accuracy_score(y_test, y_pred)   \n",
    "\n",
    "# 2nd Way: confusion_matrix(): UPSIDES: More Complete Picture, More Classification metrics (OPTIMAL)\n",
    "# Model Evaluation Metric: Confusion Matrix - A NxN matrix where N is the number of different classes of targets, showing the actual value and what it was predicted\n",
    "metrics.confusion_matrix(y_test, y_pred)                                                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2x2 Confusion Matrix where 0 is the positive and 1 the negative displays the following informations:<br>\n",
    "\n",
    "<img src=\"https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/60900/versions/12/screenshot.png\",width=400,height=400>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# True Positive\n",
    "TP = metrics.confusion_matrix(y_test, y_pred_class)[0, 0]\n",
    "\n",
    "# True Negative\n",
    "TN = metrics.confusion_matrix(y_test, y_pred_class)[1, 1]\n",
    "\n",
    "# False Positive\n",
    "FP = metrics.confusion_matrix(y_test, y_pred_class)[0, 1]\n",
    "\n",
    "# False Negative\n",
    "FN = metrics.confusion_matrix(y_test, y_pred_class)[1, 0]\n",
    "\n",
    "# Classification Accuracy from confusion matrix. SAME AS: metrics.accuracy_score(y_test, y_pred) \n",
    "accuracy_score = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "# Classification Error (or Misclassification Rate) from confusion matrix. SAME AS: 1 - metrics.accuracy_score(y_test, y_pred) \n",
    "error_score = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "# Sensitivity or True Positive Rate or Recall (i.e when the actual value is positive, how often is the prediction correct?) from confusion matrix. SAME AS: metrics.recall_score(y_test, y_pred_class)\n",
    "sensitivity = TP / float(TP + FN)\n",
    "\n",
    "# Specificity (i.e when the actual value is negative, how often is the prediction correct?) NO METRIC FUNCTION FOR THAT\n",
    "specificity = NP / float(TN + FP)\n",
    "\n",
    "# False Positive Rate = 1 - Specificity (i.e when the actual value is negative, how often is the prediction incorrect?) NO METRIC FUNCTION FOR THAT\n",
    "false_positive_rate = FP / float(TN + FP) \n",
    "\n",
    "# Precision (i.e when a positive value is predicted, how often is the prediction correct?) SAME AS: metrics.precision_score()\n",
    "precision = FP / float(TN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - K-FOLD VALIDATION\n",
    "\n",
    "A better way to evaluate the model is instead of doing the splitting of the data to train and test dataset just once, to do it many times. <br>\n",
    "The way it works, is that we split the dataset to K equal partitions (FOLDS), and each time we use 1 of it for test, and the union of the rest for training. Usually we pick K=10. In the end all of them will have been used as training and test sets, and the average accuracy score will be the one of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This function, splits the data X and y into folds, and runs the classifier clf for all of them as tests and training sets. It returns a Numpy array with the different scores\n",
    "from sklearn.cross_validation import cross_val_score  \n",
    "\n",
    "scores = cross_val_score(clf, X, y,\n",
    "                         cv=10,                    # Number of folds that the dataset is split (10-Fold Validation)\n",
    "                         scoring = 'accuracy',     # We use classification accuracy\n",
    "                         n_jobs = -1)              # Run computations in parallel for faster\n",
    "\n",
    "# The average of the values\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this we can actually calculate which K-Neighbor's parameters are the best option for our model, by checking the scores for all different K's. <br>\n",
    "BE CAREFUL: After finding the best parameters, we have to train the model, with the these best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1st Way (Manually)\n",
    "k_range = range(1,31)\n",
    "k_scores = {}\n",
    "for k in k_range:\n",
    "clf = KNeighborsClassifirer(n_neighbors=k)\n",
    "scores = cross_val_score(clf, X, y, cv=10, scoring = 'accuracy', n_jobs = -1)\n",
    "k_scores[k] = scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2nd Way (By using GridSearchCV function, which can include multi-checking for tuning) (COMPUTATIONALY EXPENSIVE)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "    \n",
    "k_range = range(1,31)\n",
    "weight_options=['uniform', 'distance']\n",
    "\n",
    "# Create a dictionary for all K's of K-Neighbors, and all weight options we want to examine for the GridSearchCV\n",
    "param_grid = dict(n_neighbors=k_range, weights=[])\n",
    "\n",
    "# Feed the function with the clf, and the dictionary instead of the data. cv specifies the number of K-Folds\n",
    "grid = GridSearchCV(clf, param_grid, cv=10, scoring = 'accuracy', n_jobs = -1)                \n",
    "\n",
    "# Fit the model with the data (it will run the cross validation for all K's, i.e cv's)\n",
    " grid.fit(X,y)\n",
    "\n",
    "# Shows a list of ALL the results including the parameters for mean and standard deviation for each K of K-Neighbors and each weight_option\n",
    "grid.grid_scores_\n",
    "\n",
    "# Shows only the parameter K of K-Neighbors for the first neighbor (i.e K=1) and both weight_options    \n",
    "grid.grid_scores_.[0]cv_validation_scores\n",
    "\n",
    "# Shows all the results from K-folds (i.e the for all 10 cv's) for the first neighbor (K=1) and both weight_options\n",
    "gird.grid_scores_.[0]parameters  \n",
    "\n",
    "# Shows the mean values from all above scores for the first neighbor (K=1) and both weight_options\n",
    "grid.grid_scores_.[0]mean_validation_score\n",
    "\n",
    "# The best mean values among all parameters of K-Neighbors and weight_options\n",
    "grid.best_score_\n",
    "\n",
    "# The parameters of K-Neighbors and weight_option that created this score\n",
    "grid.best_params_\n",
    "\n",
    "# All the details of the model of the best estimator (here K-Neighbors and weight_option)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3rd Way (By using RandomizedSearchCV function, which can include multi-checking for tuning) (COMPUTATIONALY INEXPENSIVE)\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "k_range = range(1,31)\n",
    "weight_options=['uniform', 'distance']\n",
    "param_grid = dict(n_neighbors=k_range, weights=[])\n",
    "\n",
    "# Exactly the same thing. \n",
    "# The difference: instead of running ALL possible combinations, it runs only as many as the n_iter defines, in a random way.\n",
    "rand = RandomizedSearchCV(clf, param_grid, cv=10, scoring = 'accuracy', n_iter=10)             \n",
    "\n",
    "# The best mean values among all the parameters of K-Neighbors that RandomizedSearchCV chose randomly.\n",
    "rand.best_score_\n",
    "\n",
    "# The parameters of K-Neighbors and weight_option that created this score\n",
    "rand.best_params_\n",
    "\n",
    "# All the details of the model of the best estimator (here K-Neighbors and weight_option)\n",
    "rand.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "...          \n",
    "clf = LogisticRegression()\n",
    "...\n",
    "\n",
    "# The interception term of the logistic regression with the y-axis. (The _ is for estimated attributes)   \n",
    "# y = INTERCEPT + ...\n",
    "clf.intercept_                                                                                     \n",
    "\n",
    "# The coefficients of the logistic regression (slope of line for each feature if more than one) \n",
    "# y = intercept + COEF X1 + COEF X2 + ...\n",
    "clf.coef_ \n",
    "\n",
    "# It prints both features and corresponding slopes of the line for us to be easy to know what is what  \n",
    "zip(feature_cols, clf_coef)                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1), stratify=y)  \n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=1.0,                            # How important is the violation of the margin. Adds value to Î¾. Small C means the errors matter less.\n",
    "              kernel='rbf',                     # The kernel ti be used: rbf = radius bases function, poly = polynomial, linear = linear, sigmoid = sigmoid\n",
    "              degree=1,                         # The degree of polynomial in case kernel='poly'. Ignored by any other kernel\n",
    "              decision_function_shape='ovr')    # The way to deal with more than once classes. ovr = One versus rest. ovo = one versus other\t\t\t\t\t\t\t\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "# Indices of support vectors\n",
    "clf.support_\n",
    "\n",
    "# The support vectors\n",
    "clf.support_vectors_ \n",
    "\n",
    "# The number of support vectors\n",
    "clf.n_support_ \n",
    "\n",
    "# The b from the support vector equation wx +b = 0\n",
    "clf.intercept_ \n",
    "\n",
    "# The w from the support vector equation wx +b = 0\n",
    "clf.coef_                                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=1)           \n",
    "\n",
    "clf = LinearRegression(n_jobs=-1) \n",
    "clf.fit(X_train, y_train)\n",
    "clf.predict(new_data)\n",
    "\n",
    "# The interception term of the linear regression with the y-axis\n",
    "# y = INTERCEPT + ...\n",
    "clf.intercept_\n",
    "\n",
    "# The coefficients of the linear regression (slope of line for each feature if more than one)          \n",
    "# y = intercept + COEF X1 + COEF X2 + ...\n",
    "clf.coef_ \n",
    "\n",
    "# It prints both features and corresponding slopes of the line for us to be easy to know what is what  \n",
    "zip(feature_cols, clf_coef)                                                                        \n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Evaluation Metric: Mean Absolute Error \n",
    "# In regression analysis, simply comparison does not work. We should calculate the mean difference between prediction and real value. \n",
    "# Here is the sum of (y_real - y_predict) divided by the sample size\n",
    "metrics.mean_absolute_error(y_test, clf_predict(X_test))                                           \n",
    "\n",
    "# Model Evaluation Metric: Mean Squared Error - This is simply the sum of differences (y_real - y_predict)^2 divided by the sample size\n",
    "metrics.mean_squared_error(y_test, clf_predict(X_test))\n",
    "\n",
    "# Model Evaluation Metric: Root Mean Squared Error - We could also take the sqrt of this (MOST POPULAR!) Same units as target\n",
    "np.sqrt(metrics.mean_squared_error(y_test, clf_predict(X_test)))\n",
    "                                             \n",
    "# K-Fold Validation\n",
    "from sklearn.cross_validation import cross_val_score  \n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=10,\n",
    "                         scoring = 'mean_squared_error')      # We use mean_squared_error for logistic regression \n",
    "\n",
    "# Loss function (negative likelihood)\n",
    "scores = -scores                                              \n",
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
